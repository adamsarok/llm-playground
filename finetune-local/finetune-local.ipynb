{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track & push - disable if just putzing around\n",
    "LOG_TO_WANDB = False\n",
    "PUSH_TO_HUB = False\n",
    "\n",
    "# Constants\n",
    "\n",
    "BASE_MODEL = \"google/gemma-3-270m\"\n",
    "PROJECT_NAME = \"test-finetune-local\"\n",
    "HF_USER = \"adamsarok\" \n",
    "\n",
    "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\" # put stuff here to identify the run - \"more dropout etc. ?\"\n",
    "\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "# Hyper-parameters - overall\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "MAX_SEQUENCE_LENGTH = 128 # max token length per input\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "# Hyper-parameters - QLoRA\n",
    "\n",
    "QUANT_4_BIT = True\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = LORA_R * 2\n",
    "ATTENTION_LAYERS = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "MLP_LAYERS = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "TARGET_MODULES = ATTENTION_LAYERS # ATTENTION_LAYERS + MLP_LAYERS\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Hyper-parameters - training\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "WARMUP_RATIO = 0.01 # how long it takes to reach max learning rate 1e-4\n",
    "LR_SCHEDULER_TYPE = 'cosine' # starts high and drops off\n",
    "WEIGHT_DECAY = 0.001\n",
    "OPTIMIZER = \"paged_adamw_8bit\" # \"paged_adamw_32bit\" # Adam with Weight Decay\n",
    "\n",
    "capability = torch.cuda.get_device_capability()\n",
    "use_bf16 = capability[0] >= 8\n",
    "\n",
    "# Tracking\n",
    "\n",
    "VAL_SIZE = 500\n",
    "LOG_STEPS = 5\n",
    "SAVE_STEPS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Weights & Biases\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "wandb.login()\n",
    "\n",
    "# Configure Weights & Biases to record against our project\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Simple, clean training examples\n",
    "dataset = [\n",
    "    {\"text\": \"The sky is blue.\"},\n",
    "    {\"text\": \"Cats are pets.\"},\n",
    "    {\"text\": \"Water is wet.\"},\n",
    "    {\"text\": \"The sun is hot.\"},\n",
    "]\n",
    "\n",
    "train = Dataset.from_list(dataset * 100)  # 40 samples\n",
    "val = Dataset.from_list(dataset[:2])     # 2 samples\n",
    "test = Dataset.from_list(dataset[2:])    # 2 samples\n",
    "\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "\n",
    "print(f\"Train example: {train[0]}\")\n",
    "print(f\"Val example: {val[0]}\")\n",
    "print(f\"Test example: {test[0]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATASET = \"\"\n",
    "if INPUT_DATASET is not None and INPUT_DATASET != \"\":\n",
    "    DATASET_NAME = f\"{HF_USER}/{INPUT_DATASET}\" # hugging face dataset name\n",
    "\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    train = dataset['train']\n",
    "    val = dataset['val'].select(range(VAL_SIZE))\n",
    "    test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer and the Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Parameters\n",
    "\n",
    "lora_parameters = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "train_parameters = SFTConfig(\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    optim=OPTIMIZER,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=10,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    fp16=not use_bf16,\n",
    "    bf16=use_bf16,\n",
    "    max_grad_norm=0.5,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
    "    run_name=RUN_NAME,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    save_strategy=\"steps\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_MODEL_NAME,\n",
    "    hub_private_repo=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=SAVE_STEPS,\n",
    "    gradient_checkpointing=True,  # mem optimization\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},  # mem optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    peft_config=lora_parameters,\n",
    "    args=train_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune!\n",
    "fine_tuning.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push our fine-tuned model to Hugging Face\n",
    "if PUSH_TO_HUB:\n",
    "    fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
    "    print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_TO_WANDB:\n",
    "  wandb.finish()\n",
    "  print(f\"Finished logging to WandB for run: {PROJECT_RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test our fine-tuned model from local - final \n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "checkpoint_path = os.path.abspath(\"test-finetune-local-2025-12-21_19.05.57\") #/checkpoint-50\n",
    "\n",
    "# Load LoRA adapter from local checkpoint\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    checkpoint_path  # your output directory\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer(\"The sky is\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
